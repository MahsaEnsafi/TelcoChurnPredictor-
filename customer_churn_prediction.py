# -*- coding: utf-8 -*-
"""Customer churn prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pU3XpOs9svjp7bXFDLWhZx9P8zBiC5jH
"""
#"""Customer churn prediction with SMOTE for handling class imbalance."""
#---------------------------------------------------------------------------

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')
#-------------------------------------------------------------------------==

#Import necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
import seaborn as sn
import tensorflow as tf
from tensorflow import keras
from tensorflow import keras
from imblearn.over_sampling import SMOTE
#--------------------------------------------------------------------------------

# Load the dataset
df=pd.read_csv('path.csv')
print(df.shape)  # Print the shape of the dataframe
df.sample(5)  # Display five randomly selected rows
#---------------------------------------------------------------------------------

# Data cleaning
# Drop the 'customerID' column as it is not useful for prediction
print(df.shape)  # Print the shape after dropping the column
df.sample(3)  # Display three random rows

#Converting the data type of the 'TotalCharges' column from an object to a numerical format
df[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]

# Delete rows that have blank values in 'TotalCharges'
df1=df[df.TotalCharges!=' ']
df1.shape # Check the shape after removing blanks


#Converting the data type of the 'TotalCharges' column from an object to a numerical format
df1.TotalCharges=pd.to_numeric(df1.TotalCharges)
df1.TotalCharges.dtype
#-------------------------------------------------------------------------------

# Visualization of MonthlyCharges based on Churn
Partner_Churn_no=df1[df1.Churn=='No'].MonthlyCharges
Partner_Churn_yes=df1[df1.Churn=='Yes'].MonthlyCharges
plt.xlabel('MonthlyCharges')
plt.ylabel('Number')
plt.title('Visualizition')
plt.hist([Partner_Churn_no,Partner_Churn_yes],color=['red','green'],label=['churn=no','chrn=yes'])
plt.legend()
#-------------------------------------------------------------------------------------

# Function to show unique values in object-type columns
def show_column(df1):
  for column in df1:
    if df1[column].dtype=='object':
      print(column)
      print(df1[column].unique())
show_column(df1)# Call the function to display unique values
#--------------------------------------------------------------------------------

## Replace 'No phone service' and 'No internet service' with 'No'
df1.replace('No phone service','No',inplace=True)
df1.replace('No internet service','No',inplace=True)
print(df1.shape)  # Print the updated shape
show_column(df1)  # Check unique values again
#---------------------------------------------------------------------------------

## List of yes/no columns to encode
yes_no_columns=['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup','DeviceProtection',
'TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']
dict_yes_no={'Yes':1,'No':0}

# Encode yes/no columns
for col in yes_no_columns:
  df1[col].replace(dict_yes_no,inplace=True)
  print(col)
  print(df1[col].unique())

# Encode gender column
dict_fem_mal={'Female':1,'Male':0}
df1['gender'].replace(dict_fem_mal,inplace=True)
for col in df1:
  print(col)
  print(df1[col].unique())

# One-hot encoding for categorical features
df2=pd.get_dummies(data=df1,columns=['InternetService','Contract','PaymentMethod'],dtype=np.float64)
print(df2.shape)  # Print the shape after one-hot encoding
df2.columns  # Display column names
#----------------------------------------------------------------------------------------

# Display column names and values
for col in df2:
  print(col)
  print(df2[col].unique())

# Display column types
df2.dtypes
#-----------------------------------------------------------------------------------------

# Scale numerical features
scaler=MinMaxScaler()
colmns_to_scale=['tenure','MonthlyCharges','TotalCharges']
df2[colmns_to_scale]=scaler.fit_transform(df2[colmns_to_scale])

# Count class distribution
df2['Churn'].value_counts()
#-----------------------------------------------------------------------------------------

# Define features (X) and target (Y)
X=df2.drop('Churn',axis=1)
Y=df2['Churn']
#-----------------------------------------------------------------------------------------

# Apply SMOTE to handle class imbalance
smote = SMOTE(sampling_strategy='minority')
X_S, Y_S = smote.fit_resample(X, Y)
#-----------------------------------------------------------------------------------------

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X_S, Y_S, test_size=0.2, random_state=5)

print(X_train.shape)  # Print shape of training set
print(X_test.shape)   # Print shape of test set
print(Y_train.shape)  # Print shape of training labels
print(Y_test.shape)   # Print shape of test labels
#-------------------------------------------------------------------------------------------

# Function to create and compile the model
def my_model():
    model = keras.Sequential([
        keras.layers.Dense(26, input_shape=(26,), activation='relu'),
        keras.layers.Dense(100, activation='relu'),
        keras.layers.Dense(75, activation='relu'),
        keras.layers.Dense(50, activation='relu'),
        keras.layers.Dense(25, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
    ])
    model.compile(
      optimizer='adam',
      loss=keras.losses.BinaryCrossentropy,
      metrics=['accuracy','precision','recall','f1_score']
  )
  return model
    )
    return model

# Function to evaluate the model
def eval(model, X, Y):
    model.evaluate(X, Y)

# Function for making predictions
def prediction(model, X, Y):
    Y_predict = model.predict(X)
    Y_pred = [1 if label >= 0.5 else 0 for label in Y_predict]  # Thresholding at 0.5
    print('Predicted the first five labels:', Y_pred[:5])
    print('True labels of the first five elements:', Y[:5])
    return Y_pred

# Function to display classification report and confusion matrix
def report(truth, predictions):
    print(classification_report(truth, predictions))
    cm = tf.math.confusion_matrix(labels=truth, predictions=predictions)
    plt.figure(figsize=(10, 7))
    sn.heatmap(cm, annot=True, fmt='d')
    plt.xlabel('Predicted')
    plt.ylabel('Truth')

# Function to plot training and validation loss
def plot_loss(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.show()

# Function to plot training and validation accuracy
def plot_accuracy(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

# Create and train the model
model = my_model()
history = model.fit(X_train, Y_train, epochs=40, validation_split=0.2)

# Plot the loss and accuracy
plot_loss(history)
plot_accuracy(history)

# Evaluate the model on the test set
eval(model, X_test, Y_test)

# Make predictions and generate a report
preds = prediction(model, X_test, Y_test)
report(Y_test, preds)
#---------------------------------------------------------------------------------------


model=my_model()
history=model.fit(X_train,Y_train,epochs=40,validation_split=0.2)
eval(model,X_test,Y_test)
preds=prediction(model,X_test,Y_test)
report(Y_test,preds)
